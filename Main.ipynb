{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G-ST-GP Model for HSP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "dataset = 'ven-dis' # Dataset\n",
    "N_h = 200 # Tr-set size: 50 / 100\n",
    "noise = 1e-2 # Data noise: 1e-2 / 5e-2 / 1e-1\n",
    "\n",
    "method = 'lap' # Spatial kernel construction: 'lap' / 'eu' / 'eu-dkl' / 'TIGP'\n",
    "if method == 'eu-dkl':\n",
    "  # D_in = X.shape[1] = 3\n",
    "  D_hidden = 4 # Define hidden layer in eu-dkl\n",
    "  D_out = 2 # Define output layer in eu-dkl\n",
    "elif method == 'lap':\n",
    "  eigen = 256 # Eigen-pairs for constructing laplacian matrix: 32 / 64 / 128 / 256 / 512 / 1024 / 1093 \n",
    "\n",
    "seed_g = 1 # Random seed group\n",
    "\n",
    "# fname = '_vis_healthy_ran3_tr50_noi0.02_physics_Col200.mat' # Save res as a '.mat' file named by \"fname\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><b><font size=\"6\"> Part 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt # for plotting\n",
    "# from sklearn import manifold \n",
    "import scipy.io as sio # for loading .mat files\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import time # for timing\n",
    "import sys # for printing progress+\n",
    "# from scipy.optimize import minimize, fmin_l_bfgs_b # for optimizing\n",
    "# from autograd.numpy.linalg import inv # for matrix inversion\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time # for recording running time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <font color='green'><b><font size=\"6\"> Part 2: Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X` : heart's spatial coordinates (x, y, z) | shape: (#,3)\\\n",
    "- `t`: time steps at each spatial point | shape: (#, 1)\n",
    "- `hsp`: heart's HSP data at each spatial point and each time step | shape: (#-s, #-t)\n",
    "- `Tri`: each triangle's note index (Node1-index, Node2-index, Node3-index) | shape: (#, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "print (\"Load ven-dis dataset...........\")\n",
    "inc = sio.loadmat(r'./Ven-dis input/heart_coordinates.mat')\n",
    "X = inc['heart_coordinates'] # Shape: (1094,3)\n",
    "X_shape = X.shape[0] \n",
    "\n",
    "t = sio.loadmat(r'./Ven-dis input/time_diseased.mat')\n",
    "t = t['time_diseased'].T\n",
    "t_shape = t.shape[0] # Shape: 1981\n",
    "\n",
    "hsp = sio.loadmat(r'./Ven-dis input/hsp_diseased.mat')\n",
    "hsp = hsp['hsp_diseased']\n",
    "hsp_m = hsp.T # Shape: (t, sp) = (1981, 1094)\n",
    "\n",
    "mat_data = sio.loadmat('./Ven-dis input/Ven.mat')\n",
    "Tri = mat_data['Ven'][0, 0][1]\n",
    "Tri = Tri - 1\n",
    "Tri = Tri.astype(np.int32) # Shape: (2184, 3)\n",
    "print(\"Finishing................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Prepare for training and test datasets -----\n",
    "# Add noise to all datasets\n",
    "if seed_g == 1:\n",
    "    np.random.seed(123) # 123; 456; 789; 101; 1234\n",
    "elif seed_g == 2:\n",
    "    np.random.seed(456)\n",
    "elif seed_g == 3:\n",
    "    np.random.seed(789)\n",
    "elif seed_g == 4:\n",
    "    np.random.seed(101)\n",
    "elif seed_g == 5:\n",
    "    np.random.seed(1234)\n",
    "\n",
    "Y_noise = hsp + noise * np.random.randn(hsp.shape[0], hsp.shape[1]) # Y with Gaussian noise\n",
    "\n",
    "# Training data\n",
    "np.random.seed(1234)\n",
    "\n",
    "idx = np.random.choice(X_shape, N_h, replace=False)\n",
    "idx = np.sort(idx)\n",
    "\n",
    "# Training dataset\n",
    "Y_train_noise = Y_noise[idx,:]\n",
    "print(\"Y_train_noise_shape:\", Y_train_noise.shape)\n",
    "\n",
    "# Test dataset\n",
    "mask = np.ones(hsp.shape[0], dtype=bool)\n",
    "mask[idx] = False\n",
    "Y_test_noise = Y_noise[mask,:]\n",
    "print(\"Y_test_noise_shape:\", Y_test_noise.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><b><font size=\"6\"> Part 3: Kernel Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "| Kernel | Space | Time |\n",
    "|----------|----------|----------|\n",
    "| Train-Train(K) | K_sp | K_t |\n",
    "| Train-Test | K_sp_s.T | K_t_s.T |\n",
    "| Test-Train(K*) | K_sp_s | K_t_s |\n",
    "| Test-Test(K**) | K_sp_ss | K_t_ss |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Kernel\n",
    "Matern 3/2 kernel:\n",
    "$$\n",
    "\\mathcal{K}(\\mathbf{t}_i, \\mathbf{t}_j) = \\sigma_a \\left( 1 + \\frac{\\sqrt{3}\\|\\mathbf{t}_i - \\mathbf{t}_j\\|}{l_{t}} \\right) \\exp\\left( - \\frac{\\sqrt{3}\\|\\mathbf{t}_i - \\mathbf{t}_j\\|}{l_{t}} \\right)\n",
    "$$\n",
    "\n",
    "where $l_{t}$(par[3]) for length scale, $\\sigma_a$(par[4]) for kernel scale, $\\sigma_b$(par[5]) for noise (just in $\\mathcal{K}_\\text{tr-tr}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload Kernel_Time.py\n",
    "# import importlib\n",
    "# import Kernel_Time\n",
    "# importlib.reload(Kernel_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import temporal kernel functions..................\")\n",
    "\n",
    "from Kernel_Time import cal_t_kers\n",
    "\n",
    "ker = 1.5 # 1 for Rational Quadratic Kernel ; 0.5 for Matern(1/2); 1.5 for Matern(3/2); 2.5 for Matern(5/2)\n",
    "func_K_t, func_K_t_s, func_K_t_ss = cal_t_kers(t,ker) # K_t:train-train; K_t_s: test-train; K_t_ss: test-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method: Laplacian-based\n",
    "\n",
    "Laplacian matrix $L$ is calculated as:\n",
    "$$\n",
    "M(\\nabla^2)_i = \\frac{1}{2A_i} \\sum_{j \\in N(i)} \\left( \\cot a_{ij} + \\cot b_{ij} \\right)\n",
    "$$\n",
    "Where:\n",
    "\n",
    "- $ \\nabla^2 $ is the Laplacian operator's matrix representation at vertex $ i $.\n",
    "- $ A_i $ represents the area associated with vertex $ i $.\n",
    "- $ N(i) $ refers to the set of neighboring vertices of vertex $ i $.\n",
    "- $ a_{ij} $ and $ b_{ij} $ are angles opposite to the edge between vertices $ i $ and $ j $ in a triangular mesh.\n",
    "\n",
    "[Ref]Sorkine O. 2005 Laplacian Mesh Processing, Eurographics - State of the Art Reports, no. Section 4, pp. 53–70.\n",
    "\n",
    "\n",
    "Laplacian eigenpairs kernel $\\mathcal{K}(\\mathbf{x}_i, \\mathbf{x}_j)$ is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{K} (\\mathbf{x}_i, \\mathbf{x}_j) = \\sum_{k=1}^{M} \\sigma_{m}S(\\sqrt{\\lambda_k}) \\phi_k(\\mathbf{x}_i) \\phi_k(\\mathbf{x}_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "S(w) := \\frac{2^D \\pi^{D/2} \\Gamma(\\nu + D/2)(2\\nu)^\\nu}{\\Gamma(\\nu) l_{s}^{2\\nu}} \\left( \\frac{2\\nu}{l_{s}^2} + 4\\pi^2 \\omega^2 \\right)^{-(\\nu + D/2)}\n",
    "$$\n",
    "\n",
    "where $l_{s}$(par[0]) for length scale, $\\sigma_m$(par[1]) for kernel scale, $ν$ for smoothness, $D$ as dimensionality (D = 2 for a 2D manifold in 3D space) and $Γ$ is the Gamma function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method: Euclidean-based\n",
    "\n",
    "Matern 3/2 kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method: Deep kernel learning based on Euclidean kernel\n",
    "\n",
    "Transform the input spatial coordinates into another space, then apply the spatial kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'lap':\n",
    "  print(\"Import lap spatial kernel functions..................\")\n",
    "\n",
    "  print(\"Solve eigenproblems for all data..................\")\n",
    "\n",
    "  from Eigen_Lap import eigen_sol\n",
    "\n",
    "  Q, V = eigen_sol(X, Tri, num = 256) \n",
    "  # Q: eigenvalues, shape (num,)\n",
    "  # V: eigenvectors, shape (N_verts, num)\n",
    "\n",
    "  from Kernel_Space_Lap import cal_sp_kers\n",
    "\n",
    "  func_K_sp, func_K_sp_s, func_K_sp_ss = cal_sp_kers(vertex=idx, Q=Q, V=V, kernel_type='Matern', smoothness=3./2.) # K_sp: train-train; K_sp_s: test-train; K_sp_ss: test-test\n",
    "elif method == 'eu':\n",
    "  print(\"Import euclidean spatial kernel functions..................\")\n",
    "  from Kernel_Space_Eu import calculate_spatial_kernels_euclidean\n",
    "  func_K_sp, func_K_sp_s, func_K_sp_ss = calculate_spatial_kernels_euclidean(idx, X, Tri)\n",
    "\n",
    "elif method == 'eu-dkl': # deep kernel learning\n",
    "  print(\"Import eu-dkl spatial kernel functions..................\")\n",
    "  from Kernel_Space_Eu_DKL import calculate_spatial_kernels_euclidean\n",
    "  func_K_sp, func_K_sp_s, func_K_sp_ss = calculate_spatial_kernels_euclidean(idx, X, Tri, D_hidden, D_out,\n",
    "                                        W1=None, b1=None, W2=None, b2=None, \n",
    "                                        seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><b><font size=\"6\"> Part 4: Parameters Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`par[0]`: `ls`, length-scale for spatial kernel;\\\n",
    "`par[1]`: `sigma_m`, kernel scale;\\\n",
    "`par[2]`: a minor constant to avoid ill-conditions for spatial kernel;\\\n",
    "`par[3]`: `lt`, length-scale for temporal kernel;\\\n",
    "`par[4]`: `sigma_a`, kernel scale (fixed as 1);\\\n",
    "`par[5]`: a minor constant to avoid ill-conditions for temporal kernel.\\\n",
    "`par[6]`: `D`, noise of task.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Function:\n",
    "* Negative Log-marginal-likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}_{\\text{NLL}}(\\theta) = \\frac{1}{2} \\mathbf{y_{tr}}^T \\mathbf{\\Sigma}^{-1} \\mathbf{y_{tr}} + \\frac{1}{2} \\log |\\mathbf{\\Sigma}| + \\frac{n}{2} \\log(2\\pi) = 0.5 * ( f1 + f2 + f3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} = \\mathbf{K_s} \\otimes \\mathbf{K_t} + D \\otimes I_s \\otimes I_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload files\n",
    "# import importlib\n",
    "# import Pars_MulStarts\n",
    "# import Pars_NLL\n",
    "# import Eigen_Lap\n",
    "# import Block_Diag_Inv\n",
    "\n",
    "# importlib.reload(Pars_MulStarts)\n",
    "# importlib.reload(Pars_NLL)\n",
    "# importlib.reload(Eigen_Lap)\n",
    "# importlib.reload(Block_Diag_Inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter Estimation: Lap / Eu ---\n",
    "print(\"Begin estimating kernels' parameters.................\")\n",
    "\n",
    "if method == 'lap' or method == 'eu':\n",
    "  from Pars_MulStarts import Pars_Optimizer\n",
    "  from Err_Cross_Val import cal_val_err\n",
    "\n",
    "  if method == 'lap':\n",
    "    bounds = ((0.5, 1.5), (800, 900), (0.0005, 0.0005), (50, 60), (1, 1), (0.01, 0.01), (0.0005, 2e-2))\n",
    "  elif method == 'eu':\n",
    "    bounds = ((10, 20), (80, 90), (0.0005, 0.0005), (10, 20), (1, 1), (0.01, 0.01), (0.0005, 2e-2))\n",
    "\n",
    "  starts = 1\n",
    "  np.random.seed(1234)\n",
    "  guesses = np.vstack([np.random.uniform(bound[0], bound[1], size=starts) for bound in bounds]).T\n",
    "\n",
    "  Optimizer = Pars_Optimizer(\n",
    "      Y_train_noise, Y_test_noise, \n",
    "      func_K_t, func_K_t_s, func_K_t_ss,\n",
    "      func_K_sp, func_K_sp_s, func_K_sp_ss,\n",
    "      X, Tri, mask, t\n",
    "  ) \n",
    "\n",
    "  best_pars, best_err = None, float('inf')\n",
    "  best_opt_elapsed_time = None\n",
    "  best_iter_count = None\n",
    "  for i, guess in enumerate(guesses):\n",
    "      print(f\"\\nOptimization run {i+1}:\")\n",
    "      print(f\"Initial guess {i+1}: {np.array2string(guess, separator=', ')}\")\n",
    "      opt_start_time = time.time()\n",
    "      pars, iter_count = Optimizer.opt_pars(guess, bounds)\n",
    "      opt_end_time = time.time()\n",
    "      opt_elapsed_time = opt_end_time - opt_start_time\n",
    "      print(f\"Optimized parameters: {np.array2string(pars, separator=', ')}\")\n",
    "      print(f\"Elapsed time for optimization run {i+1}: {opt_elapsed_time:.2f} seconds\")\n",
    "      # val_err = cal_val_err(pars, Y_train_noise, func_K_t, func_K_t_s, func_K_t_ss, func_K_sp, func_K_sp_s, func_K_sp_ss)\n",
    "      val_err = 0 # NOTE: for quick testing\n",
    "      if val_err < best_err:\n",
    "          best_err, best_pars = val_err, pars\n",
    "          best_opt_elapsed_time = opt_elapsed_time\n",
    "          best_iter_count = iter_count\n",
    "\n",
    "  print(\"\\nBest optimization result:\")\n",
    "  print(f\"Best parameters: {np.array2string(best_pars, separator=', ')}\")\n",
    "  print(f\"Total optimization time: {best_opt_elapsed_time}\")\n",
    "\n",
    "  pars = best_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameter Estimation: Eu-dkl ---\n",
    "if method == 'eu-dkl':\n",
    "  from DKL.Pars_MulStarts import Pars_Optimizer\n",
    "\n",
    "  D_in = 3\n",
    "  num_W1 = D_in * D_hidden\n",
    "  num_b1 = D_hidden\n",
    "  num_W2 = D_hidden * D_out\n",
    "  num_b2 = D_out\n",
    "  nn_param_dim = num_W1 + num_b1 + num_W2 + num_b2\n",
    "\n",
    "  bounds = [\n",
    "          (10, 20),    # ls\n",
    "          (80, 90),    # sigma_m\n",
    "          (0.0005, 0.0005), # sigma_n\n",
    "          (10, 20),    # lt\n",
    "          (1, 1),      # sigma_a\n",
    "          (0.01, 0.01),# sigma_b\n",
    "          (0.0005, 2e-2), # D\n",
    "      ] + [(-1, 1)]*nn_param_dim\n",
    "\n",
    "  starts = 1\n",
    "  np.random.seed(1234)\n",
    "  guesses = []\n",
    "  for bound in bounds:\n",
    "      if bound[0] == bound[1]:\n",
    "          guesses.append([bound[0]]*starts)\n",
    "      else:\n",
    "          guesses.append(np.random.uniform(bound[0], bound[1], size=starts))\n",
    "  guesses = np.array(guesses).T\n",
    "\n",
    "  def Opt_K_sp(ls, sigma_m, sigma_n, W1, b1, W2, b2):\n",
    "      return calculate_spatial_kernels_euclidean(idx, X, Tri, D_hidden, D_out, W1=W1, b1=b1, W2=W2, b2=b2)[0](ls, sigma_m, sigma_n)\n",
    "\n",
    "  Optimizer = Pars_Optimizer(\n",
    "      Y_train_noise, Y_test_noise, \n",
    "      func_K_t, func_K_t_s, func_K_t_ss,\n",
    "      Opt_K_sp, func_K_sp_s, func_K_sp_ss,\n",
    "      X, Tri, mask, t,\n",
    "      D_in, D_hidden, D_out\n",
    "  )\n",
    "\n",
    "  best_pars, best_err = None, float('inf')\n",
    "  best_opt_elapsed_time = None\n",
    "  best_iter_count = None\n",
    "  for i, guess in enumerate(guesses):\n",
    "      print(f\"\\nOptimization run {i+1}:\")\n",
    "      print(f\"Initial guess {i+1}: {np.array2string(guess, separator=', ')}\")\n",
    "      opt_start_time = time.time()\n",
    "      pars, iter_count = Optimizer.opt_pars(guess, bounds)\n",
    "      opt_end_time = time.time()\n",
    "      opt_elapsed_time = opt_end_time - opt_start_time\n",
    "      print(f\"Optimized parameters: {np.array2string(pars, separator=', ')}\")\n",
    "      print(f\"Elapsed time for optimization run {i+1}: {opt_elapsed_time:.2f} seconds\")\n",
    "      val_err = 0 # NOTE: for quick testing\n",
    "      if val_err < best_err:\n",
    "          best_err, best_pars = val_err, pars\n",
    "          best_opt_elapsed_time = opt_elapsed_time\n",
    "          best_iter_count = iter_count\n",
    "\n",
    "  print(\"\\nBest optimization result:\")\n",
    "  print(f\"Best parameters: {np.array2string(best_pars, separator=', ')}\")\n",
    "  print(f\"Total optimization time: {best_opt_elapsed_time}\")\n",
    "\n",
    "  pars = best_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><b><font size=\"6\"> Part 5: Posterior Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GP Prediction:\n",
    "$$\n",
    "\\mathbf{Y}_* \\mid X_*, X, \\mathbf{Y} \\sim \\mathcal{N} \\left( K(X_*, X) K(X, X)^{-1} \\mathbf{Y}, \\, K(X_*, X_*) - K(X_*, X) K(X, X)^{-1} K(X, X_*) \\right)\n",
    "$$\n",
    " \n",
    "where $K = K_f \\otimes K_s \\otimes K_t + D \\otimes I_s \\otimes I_t$\n",
    "\n",
    "- Relative Error:\n",
    "\n",
    "$RE = \\frac{\\|y - \\hat{y}\\|_F}{\\|y\\|_F}$\n",
    "   \n",
    "Where $\\|\\cdot\\|_F$ is the Frobenius norm. $y$ is true values (hsp_true), $\\hat{y}$ is predicted values (hsp_pred), $n$ is number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculate posterior distribution..................\")\n",
    "\n",
    "if method == \"eu\" or method == 'lap':\n",
    "  # import Post_Pred\n",
    "  # importlib.reload(Post_Pred)\n",
    "  from Post_Pred import Post_Predictor\n",
    "\n",
    "  Predictor = Post_Predictor(\n",
    "      pars=pars,\n",
    "      func_K_t=func_K_t,\n",
    "      func_K_t_s=func_K_t_s,\n",
    "      func_K_t_ss=func_K_t_ss,\n",
    "      func_K_sp=func_K_sp,\n",
    "      func_K_sp_s=func_K_sp_s,\n",
    "      func_K_sp_ss=func_K_sp_ss\n",
    "  )\n",
    "  \n",
    "  # Record prediction time\n",
    "  post_mean_start_time = time.time()\n",
    "\n",
    "  hsp_pred = Predictor.post_mean(Y_train_noise)\n",
    "  print(\"The shape of hsp_pred:\", hsp_pred.shape) # shape: (N_time, N_space)\n",
    "\n",
    "  post_mean_end_time = time.time()\n",
    "  post_mean_elapsed_time = post_mean_end_time - post_mean_start_time\n",
    "  print(f\"Elapsed time for Predictor.post_mean: {post_mean_elapsed_time:.2f} seconds\")\n",
    "  \n",
    "  cov_diag = Predictor.post_var()\n",
    "  print(\"The shape of cov_diag\", cov_diag.shape)\n",
    "\n",
    "  # --- Visualization Data ---\n",
    "  hsp_vis = Y_noise.copy()\n",
    "  hsp_vis[mask,:] = hsp_pred.T\n",
    "\n",
    "  # --- Error Analysis ---\n",
    "  hsp_true = np.array(Y_noise[:, :])\n",
    "  hsp_pred_full = np.array(hsp_vis[:, :])\n",
    "\n",
    "  diff = hsp_true - hsp_pred_full\n",
    "  re_total = np.linalg.norm(diff.reshape(-1)) / np.linalg.norm(hsp_true.reshape(-1))\n",
    "\n",
    "  print(f\"Total RE: {re_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 'eu-dkl':\n",
    "\n",
    "  from Post_Pred import Post_Predictor\n",
    "\n",
    "  # Unpack optimized parameters\n",
    "  # Neural net params\n",
    "  offset = 7\n",
    "  W1_size = D_in * D_hidden\n",
    "  b1_size = D_hidden\n",
    "  W2_size = D_hidden * D_out\n",
    "  b2_size = D_out\n",
    "\n",
    "  W1 = pars[offset : offset + W1_size].reshape(D_in, D_hidden)\n",
    "  b1 = pars[offset + W1_size : offset + W1_size + b1_size]\n",
    "  W2 = pars[offset + W1_size + b1_size : offset + W1_size + b1_size + W2_size].reshape(D_hidden, D_out)\n",
    "  b2 = pars[offset + W1_size + b1_size + W2_size : offset + W1_size + b1_size + W2_size + b2_size]\n",
    "\n",
    "  func_K_sp, func_K_sp_s, func_K_sp_ss = calculate_spatial_kernels_euclidean(idx, X, Tri, D_hidden, D_out, \n",
    "                                          W1=W1, b1=b1, W2=W2, b2=b2, \n",
    "                                          seed=42)\n",
    "\n",
    "\n",
    "  Predictor = Post_Predictor(\n",
    "      pars=pars,\n",
    "      func_K_t=func_K_t,\n",
    "      func_K_t_s=func_K_t_s,\n",
    "      func_K_t_ss=func_K_t_ss,\n",
    "      func_K_sp=func_K_sp,\n",
    "      func_K_sp_s=func_K_sp_s,\n",
    "      func_K_sp_ss=func_K_sp_ss\n",
    "  )\n",
    "\n",
    "  # Record prediction time\n",
    "  post_mean_start_time = time.time()\n",
    "\n",
    "  hsp_pred = Predictor.post_mean(Y_train_noise)\n",
    "  print(\"The shape of hsp_pred:\", hsp_pred.shape) # shape: (N_time, N_space)\n",
    "\n",
    "  post_mean_end_time = time.time()\n",
    "  post_mean_elapsed_time = post_mean_end_time - post_mean_start_time\n",
    "  print(f\"Elapsed time for Predictor.post_mean: {post_mean_elapsed_time:.2f} seconds\")\n",
    "\n",
    "  cov_diag = Predictor.post_var()\n",
    "  print(\"The shape of cov_diag\", cov_diag.shape)\n",
    "\n",
    "  # --- Visualization Data ---\n",
    "  hsp_vis = Y_noise.copy()\n",
    "  hsp_vis[mask,:] = hsp_pred.T\n",
    "  # # save hsp_vis as a .mat file\n",
    "  # scipy.io.savemat(fname, {'hsp': hsp_vis})\n",
    "\n",
    "  # --- Error Analysis ---\n",
    "  hsp_true = np.array(Y_noise[:, :])\n",
    "  hsp_pred_full = np.array(hsp_vis[:, :])\n",
    "\n",
    "  diff = hsp_true - hsp_pred_full\n",
    "  re_total = np.linalg.norm(diff.reshape(-1)) / np.linalg.norm(hsp_true.reshape(-1))\n",
    "\n",
    "  print(f\"Total RE: {re_total:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cov_diag as cov_diag_atr-n200-noi0.01.txt\n",
    "\n",
    "# np.savetxt(\"cov_diag_atr-n200-noi0.01.txt\", cov_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><b><font size=\"6\"> Part 6: Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "reps = 30 # Total iterations\n",
    "k = 3 # Added spatial points in each iteration\n",
    "re0 = re_total \n",
    "alpha = 1e4 # Weights for balancing geo-dist and LOOCV NLPD\n",
    "type_al = 'com' # Method: 'ran' / 'var' / 'geo-dist' / 'com-fix' / 'com'\n",
    "alpha1_fix = 1/3 # Options for 'com-fix';\n",
    "# alpha1_fix = 1/3, gamma = 0.5\n",
    "# alpha1_fix = 1/2, gamma = 1\n",
    "# alpha1_fix = 3/5, gamma = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: quick start for test\n",
    "# # cov_diag = np.loadtxt(\"cov_diag_vendis-n50-noi0.01.txt\"); re_total = 0.4290\n",
    "# cov_diag = np.loadtxt(\"cov_diag_atr-n50-noi0.01.txt\"); re_total = 0.507434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ActiveLearning\n",
    "# importlib.reload(ActiveLearning)\n",
    "from ActiveLearning import active_learning\n",
    "\n",
    "\n",
    "idx_new, final_hsp_pred, final_cov_diag = active_learning(reps, idx, X, Y_noise, t, cov_diag, Q, V, pars, func_K_sp, func_K_sp_s, func_K_sp_ss, func_K_t, func_K_t_s, func_K_t_ss, re0, Y_train_noise, k, Tri, type_al, alpha, alpha1_fix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV_GP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
